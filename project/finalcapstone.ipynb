{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84adfbf",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "- The Final Project is quite similar to your Mid-Term Project with regards to its dimensional data:\n",
    "    - The Date dimension, and one other dimension of your choosing must be extracted from the MySQL adventureworks database.\n",
    "    - Data from at least one other dimension must be exported from the MySQL adventureworks database as a JSON file, which must then be uploaded to MongoDB to create a new database/collection.  \n",
    "    - Your final project notebook must then extract the data from that MongoDB database/collection to create a new dimension in your data lakehouse.\n",
    "    - Data from one at least one other dimension must be exported from the MySQL adventureworks database as a CSV file, which must then be read from your local hard disk to create a new dimension in your data lakehouse.\n",
    "- Data from your Fact Table must be exported from the MySQL adventureworks database into at least three (3) separate JSON files, which will then be read into a series of PySpark Streaming Dataframes (e.g., Bronze, Silver and Gold) to complete the Lambda architecture of your Data Lakehouse.  This solution should resemble what was demonstrated in Lab 6.\n",
    "\n",
    "Overall goals: \n",
    "- design and populate a dimensional Data Lakehouse that represents a simple business process of your choosing. \n",
    "- a dimensional Data Lakehouse provides for the post hoc summarization and historic analysis of business transactions that reflect the interaction between various entities (e.g., patients & doctors, retailers & customers)\n",
    "- shows how data can be extracted from various source systems (structured, semi-structured, unstructured), transformed (cleansed, integrated), and then loaded into a destination system that’s optimized for post hoc diagnostic analysis.\n",
    "\n",
    "CheckList:\n",
    "- A Date dimension to enable the analysis of the business process over various intervals of time (the code for creating this in MySQL and Microsoft SQL Server has already been provided for you).\n",
    "- At least 3 additional dimension tables (e.g., customers, employees, products)\n",
    "- At least 1 fact table that models the business process (e.g., sales, reservations, bookings)\n",
    "- Your solution must populate its dimensions using data originating from multiple sources:\n",
    "    - A relational database like Azure MySQL, or Azure SQL Server\n",
    "    - A NoSQL database like MongoDB Atlas, or Azure Cosmos DB\n",
    "    - Files (e.g., CSV) from a cloud-based file system; like the Databricks File System (DBFS)\n",
    "- Your solution must integrate datum of differing granularity (i.e., static and near real-time)\n",
    "- Your solution must include results that demonstrate the business value of your solution. For example, a query (SELECT statement) that summarizes transaction details by customer, product, etc\n",
    "\n",
    "1. Your solution must demonstrate at least one batch execution (i.e., use sample source data [SQL, NoSQL and file system] to demonstrate loading at least one incremental data load).\n",
    "2. Your solution must demonstrate accumulating data that originates from a real-time (streaming) data source for a predetermined interval (mini-batch), integrating it with reference data, and then using the product as a source for populating your dimensional Data Lakehouse. (i.e., implement the Databricks bronze, silver, gold architecture).\n",
    "    a. Use the Spark AutoLoader to demonstrate integrating streaming data (using separate JSON files) for at least 3 intervals. This is most easily accomplished by segmenting the Fact table source data into 3 ranges and exporting them into 3 separate JSON files. \n",
    "    b. Illustrate the relationships between the “real-time” fact data and the static reference data. This is accomplished by joining fact and dimension tables at the Silver table phase.\n",
    "3. Use a Databricks Notebook to execute all data integration, object creation and query execution.\n",
    "4. Please submit all code, and other artifacts, in a GitHub repository in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0ab0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jennifer\\spark-3.5.4-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f58acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"adventureworks\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"myPassw0rd!23\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"local\", # \"atlas\"\n",
    "    \"user_name\" : \"phb8pt\",\n",
    "    \"password\" : \"bPl4xJcGTHVBA4WG\",\n",
    "    \"cluster_name\" : \"cluster0\",\n",
    "    \"cluster_subnet\" : \"g5xao\",\n",
    "    \"db_name\" : \"adventureworks\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'data')\n",
    "data_dir = os.path.join(base_dir, 'adventureworks')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "sales_orders_stream_dir = os.path.join(stream_dir, 'sales_orders')\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"adventureworks_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "sales_orders_output_bronze = os.path.join(database_dir, 'sales_orders', 'bronze')\n",
    "sales_orders_output_silver = os.path.join(database_dir, 'sales_orders', 'silver')\n",
    "sales_orders_output_gold = os.path.join(database_dir, 'sales_orders', 'gold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d31a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Adventureworks Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969d5495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory 'c:\\\\Users\\\\Jennifer\\\\Documents\\\\DS2002\\\\ds2002capstone\\\\project\\\\spark-warehouse\\\\adventureworks_dlh.db' does not exist.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)\n",
    "#print(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba04d6d",
   "metadata": {},
   "source": [
    "Create new spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1f1678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.176:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Adventureworks Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2f0c200fd40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb11dfb",
   "metadata": {},
   "source": [
    "Create a new metadata database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df68295d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Lab 06 Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Lab 6.0');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f507a4",
   "metadata": {},
   "source": [
    "Preparing to extract dimensions from different data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c60858",
   "metadata": {},
   "source": [
    "# Extract dimensions from different data sources\n",
    "1. df_products from local\n",
    "2. dim_date from MySQL\n",
    "3. df_customers from MongoDB\n",
    "4. df_employees from MongoDB\n",
    "5. df_fact_order from MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70972f57",
   "metadata": {},
   "source": [
    "## 1. df_products from local (.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743d09c",
   "metadata": {},
   "source": [
    "Use PySpark to Read Data from a **CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb897b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>MakeFlag</th>\n",
       "      <th>FinishedGoodsFlag</th>\n",
       "      <th>Color</th>\n",
       "      <th>SafetyStockLevel</th>\n",
       "      <th>ReorderPoint</th>\n",
       "      <th>StandardCost</th>\n",
       "      <th>ListPrice</th>\n",
       "      <th>...</th>\n",
       "      <th>WeightUnitMeasureCode</th>\n",
       "      <th>Weight</th>\n",
       "      <th>DaysToManufacture</th>\n",
       "      <th>ProductLine</th>\n",
       "      <th>Class</th>\n",
       "      <th>Style</th>\n",
       "      <th>ProductSubcategoryID</th>\n",
       "      <th>ProductModelID</th>\n",
       "      <th>SellEndDate</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductID             Name ProductNumber  MakeFlag  FinishedGoodsFlag  \\\n",
       "0          1  Adjustable Race       AR-5381         0                  0   \n",
       "1          2     Bearing Ball       BA-8327         0                  0   \n",
       "\n",
       "  Color  SafetyStockLevel  ReorderPoint  StandardCost  ListPrice  ...  \\\n",
       "0  None              1000           750           0.0        0.0  ...   \n",
       "1  None              1000           750           0.0        0.0  ...   \n",
       "\n",
       "  WeightUnitMeasureCode Weight DaysToManufacture  ProductLine  Class Style  \\\n",
       "0                  None    0.0                 0         None   None  None   \n",
       "1                  None    0.0                 0         None   None  None   \n",
       "\n",
       "  ProductSubcategoryID ProductModelID  SellEndDate        ModifiedDate  \n",
       "0                    0              0          NaT 2004-03-11 10:01:36  \n",
       "1                    0              0          NaT 2004-03-11 10:01:36  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_csv = os.path.join(batch_dir, 'products.csv')\n",
    "df_dim_products = spark.read.format('csv').options(header='true', inferSchema='true').load(product_csv)\n",
    "df_dim_products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f5963",
   "metadata": {},
   "source": [
    "Make necessary transformations to the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3862d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>MakeFlag</th>\n",
       "      <th>FinishedGoodsFlag</th>\n",
       "      <th>SafetyStockLevel</th>\n",
       "      <th>ReorderPoint</th>\n",
       "      <th>StandardCost</th>\n",
       "      <th>ListPrice</th>\n",
       "      <th>...</th>\n",
       "      <th>Color</th>\n",
       "      <th>WeightUnitMeasureCode</th>\n",
       "      <th>Weight</th>\n",
       "      <th>DaysToManufacture</th>\n",
       "      <th>ProductLine</th>\n",
       "      <th>Class</th>\n",
       "      <th>Style</th>\n",
       "      <th>ProductSubcategoryID</th>\n",
       "      <th>ProductModelID</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  ProductID             Name ProductNumber  MakeFlag  \\\n",
       "0            1          1  Adjustable Race       AR-5381         0   \n",
       "1            2          2     Bearing Ball       BA-8327         0   \n",
       "\n",
       "   FinishedGoodsFlag  SafetyStockLevel  ReorderPoint  StandardCost  ListPrice  \\\n",
       "0                  0              1000           750           0.0        0.0   \n",
       "1                  0              1000           750           0.0        0.0   \n",
       "\n",
       "   ... Color WeightUnitMeasureCode Weight DaysToManufacture  ProductLine  \\\n",
       "0  ...  None                  None    0.0                 0         None   \n",
       "1  ...  None                  None    0.0                 0         None   \n",
       "\n",
       "   Class Style ProductSubcategoryID ProductModelID        ModifiedDate  \n",
       "0   None  None                    0              0 2004-03-11 10:01:36  \n",
       "1   None  None                    0              0 2004-03-11 10:01:36  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'product_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_products = df_dim_products.withColumnRenamed(\"id\", \"product_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_products.createOrReplaceTempView(\"products\")\n",
    "sql_products = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY ProductID) AS product_key\n",
    "    FROM products;\n",
    "\"\"\"\n",
    "df_dim_products = spark.sql(sql_products)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['product_key', 'ProductID', 'Name', 'ProductNumber', \n",
    "                   'MakeFlag', 'FinishedGoodsFlag', 'SafetyStockLevel', \n",
    "                   'ReorderPoint', 'StandardCost', 'ListPrice', \n",
    "                   'Size', 'SizeUnitMeasureCode', 'Color',\n",
    "                   'WeightUnitMeasureCode', 'Weight', \n",
    "                   'DaysToManufacture', 'ProductLine', \n",
    "                   'Class', 'Style', \n",
    "                   'ProductSubcategoryID', 'ProductModelID', \n",
    "                   'ModifiedDate'\n",
    "                   ]\n",
    "\n",
    "df_dim_products = df_dim_products[ordered_columns]\n",
    "df_dim_products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6397241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_products.write.saveAsTable(f\"{dest_database}.dim_products\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea4039e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|         product_key|      int|   NULL|\n",
      "|           ProductID|      int|   NULL|\n",
      "|                Name|   string|   NULL|\n",
      "|       ProductNumber|   string|   NULL|\n",
      "|            MakeFlag|      int|   NULL|\n",
      "|   FinishedGoodsFlag|      int|   NULL|\n",
      "|    SafetyStockLevel|      int|   NULL|\n",
      "|        ReorderPoint|      int|   NULL|\n",
      "|        StandardCost|   double|   NULL|\n",
      "|           ListPrice|   double|   NULL|\n",
      "|                Size|   string|   NULL|\n",
      "| SizeUnitMeasureCode|   string|   NULL|\n",
      "|               Color|   string|   NULL|\n",
      "|WeightUnitMeasure...|   string|   NULL|\n",
      "|              Weight|   double|   NULL|\n",
      "|   DaysToManufacture|      int|   NULL|\n",
      "|         ProductLine|   string|   NULL|\n",
      "|               Class|   string|   NULL|\n",
      "|               Style|   string|   NULL|\n",
      "|ProductSubcategoryID|      int|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>MakeFlag</th>\n",
       "      <th>FinishedGoodsFlag</th>\n",
       "      <th>SafetyStockLevel</th>\n",
       "      <th>ReorderPoint</th>\n",
       "      <th>StandardCost</th>\n",
       "      <th>ListPrice</th>\n",
       "      <th>...</th>\n",
       "      <th>Color</th>\n",
       "      <th>WeightUnitMeasureCode</th>\n",
       "      <th>Weight</th>\n",
       "      <th>DaysToManufacture</th>\n",
       "      <th>ProductLine</th>\n",
       "      <th>Class</th>\n",
       "      <th>Style</th>\n",
       "      <th>ProductSubcategoryID</th>\n",
       "      <th>ProductModelID</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  ProductID             Name ProductNumber  MakeFlag  \\\n",
       "0            1          1  Adjustable Race       AR-5381         0   \n",
       "1            2          2     Bearing Ball       BA-8327         0   \n",
       "\n",
       "   FinishedGoodsFlag  SafetyStockLevel  ReorderPoint  StandardCost  ListPrice  \\\n",
       "0                  0              1000           750           0.0        0.0   \n",
       "1                  0              1000           750           0.0        0.0   \n",
       "\n",
       "   ... Color WeightUnitMeasureCode Weight DaysToManufacture  ProductLine  \\\n",
       "0  ...  None                  None    0.0                 0         None   \n",
       "1  ...  None                  None    0.0                 0         None   \n",
       "\n",
       "   Class Style ProductSubcategoryID ProductModelID        ModifiedDate  \n",
       "0   None  None                    0              0 2004-03-11 10:01:36  \n",
       "1   None  None                    0              0 2004-03-11 10:01:36  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_products;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_products LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1b937",
   "metadata": {},
   "source": [
    "## 2. dim_date from MySQL\n",
    "I did include dim_date.csv in case it was needed though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f43215e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = f\"SELECT * FROM {mysql_args['db_name']}.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9102f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea368dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            date_key|      int|   NULL|\n",
      "|           full_date|     date|   NULL|\n",
      "|           date_name| char(11)|   NULL|\n",
      "|        date_name_us| char(11)|   NULL|\n",
      "|        date_name_eu| char(11)|   NULL|\n",
      "|         day_of_week|  tinyint|   NULL|\n",
      "|    day_name_of_week| char(10)|   NULL|\n",
      "|        day_of_month|  tinyint|   NULL|\n",
      "|         day_of_year|      int|   NULL|\n",
      "|     weekday_weekend| char(10)|   NULL|\n",
      "|        week_of_year|  tinyint|   NULL|\n",
      "|          month_name| char(10)|   NULL|\n",
      "|       month_of_year|  tinyint|   NULL|\n",
      "|is_last_day_of_month|  char(1)|   NULL|\n",
      "|    calendar_quarter|  tinyint|   NULL|\n",
      "|       calendar_year|      int|   NULL|\n",
      "| calendar_year_month| char(10)|   NULL|\n",
      "|   calendar_year_qtr| char(10)|   NULL|\n",
      "|fiscal_month_of_year|  tinyint|   NULL|\n",
      "|      fiscal_quarter|  tinyint|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date    date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20000101  2000-01-01  2000/01/01   01/01/2000   01/01/2000             7   \n",
       "1  20000102  2000-01-02  2000/01/02   01/02/2000   02/01/2000             1   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0       Saturday               1            1      Weekend     ...   \n",
       "1       Sunday                 2            2      Weekend     ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000          2000-01      \n",
       "1                     N                1           2000          2000-01      \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0         2000Q1                         7              3        2000   \n",
       "1         2000Q1                         7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0         2000-07          2000Q3      \n",
       "1         2000-07          2000Q3      \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d561dd",
   "metadata": {},
   "source": [
    "## 3. df_customers, df_employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6723a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "json_files = {\"customers\" : \"customers.json\",\n",
    "              \"employees\" : 'employees.json'\n",
    "             }\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], batch_dir, json_files) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffdce0",
   "metadata": {},
   "source": [
    "### 3a. df_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea78d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>TerritoryID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AW00000001</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AW00000002</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AccountNumber  CustomerID CustomerType         ModifiedDate  TerritoryID\n",
       "0    AW00000001           1            S  2004-10-13 11:15:07            1\n",
       "1    AW00000002           2            S  2004-10-13 11:15:07            1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"customers\"\n",
    "\n",
    "df_dim_customers = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70eb674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>TerritoryID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>AW00000001</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>AW00000002</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  CustomerID CustomerType AccountNumber         ModifiedDate  \\\n",
       "0             1           1            S    AW00000001  2004-10-13 11:15:07   \n",
       "1             2           2            S    AW00000002  2004-10-13 11:15:07   \n",
       "\n",
       "   TerritoryID  \n",
       "0            1  \n",
       "1            1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'customer_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_customers = df_dim_customers.withColumnRenamed(\"id\", \"customer_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_customers.createOrReplaceTempView(\"customers\")\n",
    "sql_invoices = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY CustomerID) AS customer_key\n",
    "    FROM customers;\n",
    "\"\"\"\n",
    "df_dim_customers = spark.sql(sql_invoices)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['customer_key', 'CustomerID','CustomerType', 'AccountNumber', \n",
    "                   'ModifiedDate', 'TerritoryID']\n",
    "df_dim_customers = df_dim_customers[ordered_columns]\n",
    "df_dim_customers.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6633f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "231b3000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        customer_key|                 int|   NULL|\n",
      "|          CustomerID|                 int|   NULL|\n",
      "|        CustomerType|              string|   NULL|\n",
      "|       AccountNumber|              string|   NULL|\n",
      "|        ModifiedDate|              string|   NULL|\n",
      "|         TerritoryID|                 int|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|       dim_customers|       |\n",
      "|        Created Time|Fri May 09 18:26:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/c:/Users/Je...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>TerritoryID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>AW00000001</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>AW00000002</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  CustomerID CustomerType AccountNumber         ModifiedDate  \\\n",
       "0             1           1            S    AW00000001  2004-10-13 11:15:07   \n",
       "1             2           2            S    AW00000002  2004-10-13 11:15:07   \n",
       "\n",
       "   TerritoryID  \n",
       "0            1  \n",
       "1            1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf9f0f",
   "metadata": {},
   "source": [
    "### 3b. df_employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6209b6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>ContactID</th>\n",
       "      <th>CurrentFlag</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HireDate</th>\n",
       "      <th>LoginID</th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>NationalIDNumber</th>\n",
       "      <th>SalariedFlag</th>\n",
       "      <th>SickLeaveHours</th>\n",
       "      <th>Title</th>\n",
       "      <th>VacationHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1972-05-15 00:00:00</td>\n",
       "      <td>1209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1996-07-31 00:00:00</td>\n",
       "      <td>adventure-works\\guy1</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>2004-07-31 00:00:00</td>\n",
       "      <td>14417807</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1977-06-03 00:00:00</td>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>1997-02-26 00:00:00</td>\n",
       "      <td>adventure-works\\kevin0</td>\n",
       "      <td>6</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-07-31 00:00:00</td>\n",
       "      <td>253022876</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BirthDate  ContactID CurrentFlag  EmployeeID Gender  \\\n",
       "0  1972-05-15 00:00:00       1209           1           1      M   \n",
       "1  1977-06-03 00:00:00       1030           1           2      M   \n",
       "\n",
       "              HireDate                 LoginID  ManagerID MaritalStatus  \\\n",
       "0  1996-07-31 00:00:00    adventure-works\\guy1         16             M   \n",
       "1  1997-02-26 00:00:00  adventure-works\\kevin0          6             S   \n",
       "\n",
       "          ModifiedDate NationalIDNumber SalariedFlag  SickLeaveHours  \\\n",
       "0  2004-07-31 00:00:00         14417807            0              30   \n",
       "1  2004-07-31 00:00:00        253022876            0              41   \n",
       "\n",
       "                          Title  VacationHours  \n",
       "0  Production Technician - WC60             21  \n",
       "1           Marketing Assistant             42  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"employees\"\n",
    "\n",
    "df_dim_employees = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_employees.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef192b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_key</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>ContactID</th>\n",
       "      <th>CurrentFlag</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HireDate</th>\n",
       "      <th>LoginID</th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NationalIDNumber</th>\n",
       "      <th>SalariedFlag</th>\n",
       "      <th>SickLeaveHours</th>\n",
       "      <th>Title</th>\n",
       "      <th>VacationHours</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1972-05-15 00:00:00</td>\n",
       "      <td>1209</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1996-07-31 00:00:00</td>\n",
       "      <td>adventure-works\\guy1</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>14417807</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "      <td>21</td>\n",
       "      <td>2004-07-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1977-06-03 00:00:00</td>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1997-02-26 00:00:00</td>\n",
       "      <td>adventure-works\\kevin0</td>\n",
       "      <td>6</td>\n",
       "      <td>S</td>\n",
       "      <td>253022876</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "      <td>42</td>\n",
       "      <td>2004-07-31 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_key  EmployeeID            BirthDate  ContactID CurrentFlag  \\\n",
       "0             1           1  1972-05-15 00:00:00       1209           1   \n",
       "1             2           2  1977-06-03 00:00:00       1030           1   \n",
       "\n",
       "  Gender             HireDate                 LoginID  ManagerID  \\\n",
       "0      M  1996-07-31 00:00:00    adventure-works\\guy1         16   \n",
       "1      M  1997-02-26 00:00:00  adventure-works\\kevin0          6   \n",
       "\n",
       "  MaritalStatus NationalIDNumber SalariedFlag  SickLeaveHours  \\\n",
       "0             M         14417807            0              30   \n",
       "1             S        253022876            0              41   \n",
       "\n",
       "                          Title  VacationHours         ModifiedDate  \n",
       "0  Production Technician - WC60             21  2004-07-31 00:00:00  \n",
       "1           Marketing Assistant             42  2004-07-31 00:00:00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'employee_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_employees = df_dim_employees.withColumnRenamed(\"id\", \"employee_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_employees.createOrReplaceTempView(\"employees\")\n",
    "sql_invoices = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY EmployeeID) AS employee_key\n",
    "    FROM employees;\n",
    "\"\"\"\n",
    "df_dim_employees = spark.sql(sql_invoices)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['employee_key', 'EmployeeID', 'BirthDate', 'ContactID', 'CurrentFlag', \n",
    "                   'Gender', 'HireDate', 'LoginID', 'ManagerID', \n",
    "                   'MaritalStatus', 'NationalIDNumber', 'SalariedFlag', \n",
    "                   'SickLeaveHours', 'Title', 'VacationHours', 'ModifiedDate']\n",
    "df_dim_employees = df_dim_employees[ordered_columns]\n",
    "df_dim_employees.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621c674",
   "metadata": {},
   "source": [
    "Save as the dim_employees table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed871d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_employees.write.saveAsTable(f\"{dest_database}.dim_employees\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8f04d",
   "metadata": {},
   "source": [
    "Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8a725fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------+\n",
      "|            col_name|         data_type|comment|\n",
      "+--------------------+------------------+-------+\n",
      "|        employee_key|               int|   NULL|\n",
      "|          EmployeeID|               int|   NULL|\n",
      "|           BirthDate|            string|   NULL|\n",
      "|           ContactID|               int|   NULL|\n",
      "|         CurrentFlag|            string|   NULL|\n",
      "|              Gender|            string|   NULL|\n",
      "|            HireDate|            string|   NULL|\n",
      "|             LoginID|            string|   NULL|\n",
      "|           ManagerID|               int|   NULL|\n",
      "|       MaritalStatus|            string|   NULL|\n",
      "|    NationalIDNumber|            string|   NULL|\n",
      "|        SalariedFlag|            string|   NULL|\n",
      "|      SickLeaveHours|               int|   NULL|\n",
      "|               Title|            string|   NULL|\n",
      "|       VacationHours|               int|   NULL|\n",
      "|        ModifiedDate|            string|   NULL|\n",
      "|                    |                  |       |\n",
      "|# Detailed Table ...|                  |       |\n",
      "|             Catalog|     spark_catalog|       |\n",
      "|            Database|adventureworks_dlh|       |\n",
      "+--------------------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_key</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>ContactID</th>\n",
       "      <th>CurrentFlag</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HireDate</th>\n",
       "      <th>LoginID</th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NationalIDNumber</th>\n",
       "      <th>SalariedFlag</th>\n",
       "      <th>SickLeaveHours</th>\n",
       "      <th>Title</th>\n",
       "      <th>VacationHours</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1972-05-15 00:00:00</td>\n",
       "      <td>1209</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1996-07-31 00:00:00</td>\n",
       "      <td>adventure-works\\guy1</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>14417807</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "      <td>21</td>\n",
       "      <td>2004-07-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1977-06-03 00:00:00</td>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1997-02-26 00:00:00</td>\n",
       "      <td>adventure-works\\kevin0</td>\n",
       "      <td>6</td>\n",
       "      <td>S</td>\n",
       "      <td>253022876</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "      <td>42</td>\n",
       "      <td>2004-07-31 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_key  EmployeeID            BirthDate  ContactID CurrentFlag  \\\n",
       "0             1           1  1972-05-15 00:00:00       1209           1   \n",
       "1             2           2  1977-06-03 00:00:00       1030           1   \n",
       "\n",
       "  Gender             HireDate                 LoginID  ManagerID  \\\n",
       "0      M  1996-07-31 00:00:00    adventure-works\\guy1         16   \n",
       "1      M  1997-02-26 00:00:00  adventure-works\\kevin0          6   \n",
       "\n",
       "  MaritalStatus NationalIDNumber SalariedFlag  SickLeaveHours  \\\n",
       "0             M         14417807            0              30   \n",
       "1             S        253022876            0              41   \n",
       "\n",
       "                          Title  VacationHours         ModifiedDate  \n",
       "0  Production Technician - WC60             21  2004-07-31 00:00:00  \n",
       "1           Marketing Assistant             42  2004-07-31 00:00:00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_employees;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_employees LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870fe942",
   "metadata": {},
   "source": [
    "# Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bc9afca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_customers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_employees</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_products</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>customers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>employees</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>products</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            namespace      tableName  isTemporary\n",
       "0  adventureworks_dlh  dim_customers        False\n",
       "1  adventureworks_dlh       dim_date        False\n",
       "2  adventureworks_dlh  dim_employees        False\n",
       "3  adventureworks_dlh   dim_products        False\n",
       "4                          customers         True\n",
       "5                          employees         True\n",
       "6                           products         True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98182661",
   "metadata": {},
   "source": [
    "#  Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Sales Orders</span> Fact Data\n",
    "8.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d4db7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sales_orders_01.json</td>\n",
       "      <td>6481093</td>\n",
       "      <td>2025-05-09 04:07:02.155543089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sales_orders_02.json</td>\n",
       "      <td>6431521</td>\n",
       "      <td>2025-05-09 04:07:31.031861067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sales_orders_03.json</td>\n",
       "      <td>6431760</td>\n",
       "      <td>2025-05-09 04:09:08.718038797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name     size             modification_time\n",
       "0  sales_orders_01.json  6481093 2025-05-09 04:07:02.155543089\n",
       "1  sales_orders_02.json  6431521 2025-05-09 04:07:31.031861067\n",
       "2  sales_orders_03.json  6431760 2025-05-09 04:09:08.718038797"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(sales_orders_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08531660",
   "metadata": {},
   "source": [
    "## Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a7bcee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_orders_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\", sales_orders_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(sales_orders_stream_dir)\n",
    ")\n",
    "\n",
    "df_sales_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07ef8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_orders_checkpoint_bronze = os.path.join(sales_orders_output_bronze, '_checkpoint')\n",
    "\n",
    "sales_orders_bronze_query = (\n",
    "    df_sales_orders_bronze\n",
    "    #  adding Current Timestamp and Input Filename columns for traceability\n",
    "    .withColumn(\"current_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"input_filename\", input_file_name()) \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"sales_orders_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", sales_orders_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(sales_orders_output_bronze)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a04418b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 6e04d728-5308-4eb8-ac94-d5dc167d1dcb\n",
      "Query Name: sales_orders_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {sales_orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {sales_orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {sales_orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bc856d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb56bb",
   "metadata": {},
   "source": [
    "## Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "- Prepare Role-Playing Dimension Primary and Business Keys\n",
    "- Integrate \"Cold-path\" Data & Make Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b343064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ContactID\n",
    "df_dim_employee_contact = df_dim_employees.select(col(\"employee_key\").alias(\"employee_key\"), col(\"EmployeeID\").alias(\"employee_contact_id\"))\n",
    "\n",
    "# CustomerID\n",
    "df_dim_customer_contact = df_dim_customers.select(col(\"customer_key\").alias(\"customer_key\"), col(\"CustomerID\").alias(\"customer_id\"))\n",
    "\n",
    "\n",
    "df_dim_due_date = df_dim_date.select(col(\"date_key\").alias(\"due_date_key\"),  col(\"full_date\").alias(\"due_full_date\"))\n",
    "\n",
    "df_dim_order_date = df_dim_date.select(col(\"date_key\").alias(\"ordered_date_key\"),  col(\"full_date\").alias(\"ordered_full_date\"))\n",
    "\n",
    "df_dim_ship_date = df_dim_date.select(col(\"date_key\").alias(\"shipped_date_key\"),  col(\"full_date\").alias(\"shipped_full_date\"))\n",
    "\n",
    "df_dim_modified_date = df_dim_date.select(col(\"date_key\").alias(\"modified_date_key\"),  col(\"full_date\").alias(\"full_modified_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "173e68ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AccountNumber: string (nullable = true)\n",
      " |-- BillToAddressID: long (nullable = true)\n",
      " |-- ContactID: long (nullable = true)\n",
      " |-- CreditCardApprovalCode: string (nullable = true)\n",
      " |-- CreditCardID: long (nullable = true)\n",
      " |-- CurrencyRateID: long (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- DueDate: string (nullable = true)\n",
      " |-- Freight: double (nullable = true)\n",
      " |-- ModifiedDate: string (nullable = true)\n",
      " |-- OnlineOrderFlag: string (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- PurchaseOrderNumber: string (nullable = true)\n",
      " |-- RevisionNumber: long (nullable = true)\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- SalesOrderNumber: string (nullable = true)\n",
      " |-- SalesPersonID: long (nullable = true)\n",
      " |-- ShipDate: string (nullable = true)\n",
      " |-- ShipMethodID: long (nullable = true)\n",
      " |-- ShipToAddressID: long (nullable = true)\n",
      " |-- Status: long (nullable = true)\n",
      " |-- SubTotal: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- TerritoryID: long (nullable = true)\n",
      " |-- TotalDue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_orders_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd829a",
   "metadata": {},
   "source": [
    "Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae6d6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_orders_silver = spark.readStream.format(\"parquet\").load(sales_orders_output_bronze) \\\n",
    "    .join(df_dim_employee_contact, df_dim_employee_contact.employee_contact_id == col(\"ContactID\").cast(IntegerType()), 'left') \\\n",
    "    .join(df_dim_customer_contact, df_dim_customer_contact.customer_id == col(\"CustomerID\").cast(IntegerType()), 'left') \\\n",
    "    .join(df_dim_due_date, df_dim_due_date.due_full_date.cast(DateType()) == col(\"DueDate\").cast(DateType()), \"left\") \\\n",
    "    .join(df_dim_order_date, df_dim_order_date.ordered_full_date.cast(DateType()) == col(\"OrderDate\").cast(DateType()), \"left\") \\\n",
    "    .join(df_dim_ship_date, df_dim_ship_date.shipped_full_date.cast(DateType()) == col(\"ShipDate\").cast(DateType()), \"left\") \\\n",
    "    .join(df_dim_modified_date, df_dim_modified_date.full_modified_date.cast(DateType()) == col(\"ModifiedDate\").cast(DateType()), \"left\") \\\n",
    "    .select(col(\"SalesOrderID\").cast(LongType()), \\\n",
    "            df_dim_employee_contact.employee_key.cast(LongType()), \\\n",
    "            df_dim_customer_contact.customer_key.cast(LongType()), \\\n",
    "            df_dim_due_date.due_date_key.cast(LongType()), \\\n",
    "            df_dim_order_date.ordered_date_key.cast(LongType()), \\\n",
    "            df_dim_ship_date.shipped_date_key.cast(LongType()), \\\n",
    "            df_dim_modified_date.modified_date_key.cast(LongType()), \\\n",
    "            col(\"AccountNumber\"), \\\n",
    "            col(\"BillToAddressID\"), \\\n",
    "            col(\"CreditCardApprovalCode\"), \\\n",
    "            col(\"CreditCardID\"), \\\n",
    "            col(\"CurrencyRateID\"), \\\n",
    "            col(\"Freight\"), \\\n",
    "            col(\"OnlineOrderFlag\"), \\\n",
    "            col(\"PurchaseOrderNumber\"), \\\n",
    "            col(\"RevisionNumber\"), \\\n",
    "            col(\"SalesOrderNumber\"), \\\n",
    "            col(\"SalesPersonID\"), \\\n",
    "            col(\"ShipMethodID\"), \n",
    "            col(\"ShipToAddressID\"), \\\n",
    "            col(\"Status\"), \\\n",
    "            col(\"SubTotal\"), \\\n",
    "            col(\"TaxAmt\"), \\\n",
    "            col(\"TerritoryID\"), \\\n",
    "            col(\"TotalDue\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "260f38d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed89aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- employee_key: long (nullable = true)\n",
      " |-- customer_key: long (nullable = true)\n",
      " |-- due_date_key: long (nullable = true)\n",
      " |-- ordered_date_key: long (nullable = true)\n",
      " |-- shipped_date_key: long (nullable = true)\n",
      " |-- modified_date_key: long (nullable = true)\n",
      " |-- AccountNumber: string (nullable = true)\n",
      " |-- BillToAddressID: long (nullable = true)\n",
      " |-- CreditCardApprovalCode: string (nullable = true)\n",
      " |-- CreditCardID: long (nullable = true)\n",
      " |-- CurrencyRateID: long (nullable = true)\n",
      " |-- Freight: double (nullable = true)\n",
      " |-- OnlineOrderFlag: string (nullable = true)\n",
      " |-- PurchaseOrderNumber: string (nullable = true)\n",
      " |-- RevisionNumber: long (nullable = true)\n",
      " |-- SalesOrderNumber: string (nullable = true)\n",
      " |-- SalesPersonID: long (nullable = true)\n",
      " |-- ShipMethodID: long (nullable = true)\n",
      " |-- ShipToAddressID: long (nullable = true)\n",
      " |-- Status: long (nullable = true)\n",
      " |-- SubTotal: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- TerritoryID: long (nullable = true)\n",
      " |-- TotalDue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d4813",
   "metadata": {},
   "source": [
    "Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad4f1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_orders_checkpoint_silver = os.path.join(sales_orders_output_silver, '_checkpoint')\n",
    "\n",
    "sales_orders_silver_query = (\n",
    "    df_sales_orders_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"sales_orders_silver\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", sales_orders_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(sales_orders_output_silver)\n",
    "    # writeStream, in 'parquet' format, to 'purchase_orders_output_silver' in 'append' mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "463f697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 4b8052e4-b7d5-41f5-bd5b-3a5051d9e57b\n",
      "Query Name: sales_orders_silver\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {sales_orders_silver_query.id}\")\n",
    "print(f\"Query Name: {sales_orders_silver_query.name}\")\n",
    "print(f\"Query Status: {sales_orders_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b11a5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1b93721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ordered_date_key</th>\n",
       "      <th>ordered_full_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000103</td>\n",
       "      <td>2000-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20000104</td>\n",
       "      <td>2000-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20000105</td>\n",
       "      <td>2000-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ordered_date_key ordered_full_date\n",
       "0          20000101        2000-01-01\n",
       "1          20000102        2000-01-02\n",
       "2          20000103        2000-01-03\n",
       "3          20000104        2000-01-04\n",
       "4          20000105        2000-01-05"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_order_date.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659964f",
   "metadata": {},
   "source": [
    "Gold Layer: Perform Aggregations\n",
    "\n",
    "Business Report Possibilities/Thoughts:\n",
    "- Identify top-performing salespeople and which regions they are excelling in.\n",
    "- Helps reward high performers, optimize salesforce allocation, and mentor underperformers.\n",
    "- Evaluate geographic market strength based on TerritoryID.\n",
    "- Number_of_Orders helps distinguish between High-volume, low-value territories/employees (many small orders) and  Low-volume, high-value ones (few but large deals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d359519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_by_product_category_gold = spark.readStream.format(\"parquet\").load(sales_orders_output_silver) \\\n",
    ".join(df_dim_employee_contact, \"employee_key\") \\\n",
    ".join(df_dim_customer_contact, \"customer_key\") \\\n",
    ".join(df_dim_due_date, \"due_date_key\") \\\n",
    ".join(df_dim_order_date, \"ordered_date_key\") \\\n",
    ".join(df_dim_ship_date, \"shipped_date_key\") \\\n",
    ".join(df_dim_modified_date, \"modified_date_key\") \\\n",
    ".groupBy(\"employee_key\", \"TerritoryID\") \\\n",
    ".agg(count(\"SalesOrderID\").alias(\"Number_of_Orders\"),\n",
    "    sum(\"TotalDue\").alias(\"Total_Revenue\"),\n",
    "    avg(\"TotalDue\").alias(\"Average_Order_Value\")) \\\n",
    ".orderBy(desc(\"Total_Revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7a4a782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_key: long (nullable = true)\n",
      " |-- TerritoryID: long (nullable = true)\n",
      " |-- Number_of_Orders: long (nullable = false)\n",
      " |-- Total_Revenue: double (nullable = true)\n",
      " |-- Average_Order_Value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_by_product_category_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122efeb",
   "metadata": {},
   "source": [
    "Writing the streaming data to a parquet file in \"complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4d71b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_orders_gold_query = (\n",
    "    df_orders_by_product_category_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_sales_orders_by_employee_territory\")\n",
    "    .start()\n",
    "    # create the new \"fact_inventory_trans_by_product\" query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84024d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 1 batchs\n"
     ]
    }
   ],
   "source": [
    "wait_until_stream_is_ready(sales_orders_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ae90b",
   "metadata": {},
   "source": [
    "Querying the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3462c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_key: long (nullable = true)\n",
      " |-- TerritoryID: long (nullable = true)\n",
      " |-- Number_of_Orders: long (nullable = false)\n",
      " |-- Total_Revenue: double (nullable = true)\n",
      " |-- Average_Order_Value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_sales_orders_by_employee_territory = spark.sql(\"SELECT * FROM fact_sales_orders_by_employee_territory\")\n",
    "df_fact_sales_orders_by_employee_territory.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e801608",
   "metadata": {},
   "source": [
    "Creating the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42b3d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_sales_orders_by_employee_territory_gold_final = df_fact_sales_orders_by_employee_territory \\\n",
    ".select(col(\"employee_key\").alias(\"Employee\"), \\\n",
    "        col(\"TerritoryID\").alias(\"Location\"), \\\n",
    "        col(\"Total_Revenue\").alias(\"Total Revenue\"), \\\n",
    "        col(\"Number_of_Orders\").alias(\"Total Number of Orders\"), \\\n",
    "        col(\"Average_Order_Value\").alias(\"Average Order Value\")) \\\n",
    ".orderBy(desc(\"Total Revenue\"), desc(\"Average Order Value\"), desc(\"Total Number of Orders\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b363f",
   "metadata": {},
   "source": [
    "Loading the final results into a new table and displaying the results (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75a96c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee</th>\n",
       "      <th>Location</th>\n",
       "      <th>Total Revenue</th>\n",
       "      <th>Total Number of Orders</th>\n",
       "      <th>Average Order Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>267</td>\n",
       "      <td>1</td>\n",
       "      <td>206180.1474</td>\n",
       "      <td>4</td>\n",
       "      <td>51545.036850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272</td>\n",
       "      <td>8</td>\n",
       "      <td>205237.8183</td>\n",
       "      <td>4</td>\n",
       "      <td>51309.454575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>204090.5716</td>\n",
       "      <td>4</td>\n",
       "      <td>51022.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>203746.6920</td>\n",
       "      <td>4</td>\n",
       "      <td>50936.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167</td>\n",
       "      <td>9</td>\n",
       "      <td>194536.3520</td>\n",
       "      <td>4</td>\n",
       "      <td>48634.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>96</td>\n",
       "      <td>7</td>\n",
       "      <td>390941.8673</td>\n",
       "      <td>4</td>\n",
       "      <td>97735.466825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "      <td>388552.7446</td>\n",
       "      <td>8</td>\n",
       "      <td>48569.093075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>245</td>\n",
       "      <td>6</td>\n",
       "      <td>386482.9440</td>\n",
       "      <td>12</td>\n",
       "      <td>32206.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>386084.6761</td>\n",
       "      <td>8</td>\n",
       "      <td>48260.584513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>384374.5877</td>\n",
       "      <td>4</td>\n",
       "      <td>96093.646925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>259 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Employee  Location  Total Revenue  Total Number of Orders  \\\n",
       "0         267         1    206180.1474                       4   \n",
       "1         272         8    205237.8183                       4   \n",
       "2         185         1    204090.5716                       4   \n",
       "3         116         1    203746.6920                       4   \n",
       "4         167         9    194536.3520                       4   \n",
       "..        ...       ...            ...                     ...   \n",
       "254        96         7    390941.8673                       4   \n",
       "255       201         2    388552.7446                       8   \n",
       "256       245         6    386482.9440                      12   \n",
       "257        33         6    386084.6761                       8   \n",
       "258        43         4    384374.5877                       4   \n",
       "\n",
       "     Average Order Value  \n",
       "0           51545.036850  \n",
       "1           51309.454575  \n",
       "2           51022.642900  \n",
       "3           50936.673000  \n",
       "4           48634.088000  \n",
       "..                   ...  \n",
       "254         97735.466825  \n",
       "255         48569.093075  \n",
       "256         32206.912000  \n",
       "257         48260.584513  \n",
       "258         96093.646925  \n",
       "\n",
       "[259 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_sales_orders_by_employee_territory_gold_final.write.saveAsTable(f\"{dest_database}.fact_sales_orders_by_employee_territory\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_sales_orders_by_employee_territory\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ee9fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
